{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "from pyimagesearch.minigooglenet import MiniGoogLeNet\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.optimizers import SGD\n",
    "from keras.datasets import cifar10\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = 2\n",
    "output = 'multi_gpus.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = num_gpus\n",
    "OUTPUT = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 70\n",
    "INIT_LR = 5e-3\n",
    " \n",
    "def poly_decay(epoch):\n",
    "    # initialize the maximum number of epochs, base learning rate,\n",
    "    # and power of the polynomial\n",
    "    maxEpochs = NUM_EPOCHS\n",
    "    baseLR = INIT_LR\n",
    "    power = 1.0\n",
    " \n",
    "    # compute the new learning rate based on polynomial decay\n",
    "    alpha = baseLR * (1 - (epoch / float(maxEpochs))) ** power\n",
    " \n",
    "    # return the new learning rate\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the training and testing data, converting the images from\n",
    "# integers to floats\n",
    "((trainX, trainY), (testX, testY)) = cifar10.load_data()\n",
    "trainX = trainX.astype(\"float\")\n",
    "testX = testX.astype(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply mean subtraction to the data\n",
    "mean = np.mean(trainX, axis=0)\n",
    "trainX -= mean\n",
    "testX -= mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the labels from integers to vectors\n",
    "lb = LabelBinarizer()\n",
    "trainY = lb.fit_transform(trainY)\n",
    "testY = lb.transform(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the image generator for data augmentation and construct\n",
    "# the set of callbacks\n",
    "aug = ImageDataGenerator(width_shift_range=0.1,\n",
    "    height_shift_range=0.1, horizontal_flip=True,\n",
    "    fill_mode=\"nearest\")\n",
    "callbacks = [LearningRateScheduler(poly_decay)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training with 2 GPUs...\n"
     ]
    }
   ],
   "source": [
    "# check to see if we are compiling using just a single GPU\n",
    "if G <= 1:\n",
    "\tprint(\"[INFO] training with 1 GPU...\")\n",
    "\tmodel = MiniGoogLeNet.build(width=32, height=32, depth=3,\n",
    "\t\tclasses=10)\n",
    "# otherwise, we are compiling using multiple GPUs\n",
    "else:\n",
    "\tprint(\"[INFO] training with {} GPUs...\".format(G))\n",
    " \n",
    "\t# we'll store a copy of the model on *every* GPU and then combine\n",
    "\t# the results from the gradient updates on the CPU\n",
    "\twith tf.device(\"/device:GPU:0\"):\n",
    "\t\t# initialize the model\n",
    "\t\tmodel = MiniGoogLeNet.build(width=32, height=32, depth=3,\n",
    "\t\t\tclasses=10)\n",
    "\t\n",
    "\t# make the model parallel\n",
    "\t#model = multi_gpu_model(model, gpus=G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n",
      "[INFO] training network...\n",
      "Epoch 1/70\n",
      " - 24s - loss: 1.5805 - acc: 0.4201 - val_loss: 1.2858 - val_acc: 0.5412\n",
      "Epoch 2/70\n",
      " - 21s - loss: 1.1781 - acc: 0.5754 - val_loss: 1.4924 - val_acc: 0.4867\n",
      "Epoch 3/70\n",
      " - 21s - loss: 1.0096 - acc: 0.6414 - val_loss: 1.1473 - val_acc: 0.5984\n",
      "Epoch 4/70\n",
      " - 21s - loss: 0.8920 - acc: 0.6832 - val_loss: 1.0776 - val_acc: 0.6385\n",
      "Epoch 5/70\n",
      " - 21s - loss: 0.8088 - acc: 0.7151 - val_loss: 0.7296 - val_acc: 0.7434\n",
      "Epoch 6/70\n",
      " - 21s - loss: 0.7398 - acc: 0.7420 - val_loss: 0.8101 - val_acc: 0.7203\n",
      "Epoch 7/70\n",
      " - 21s - loss: 0.6833 - acc: 0.7629 - val_loss: 0.8309 - val_acc: 0.7183\n",
      "Epoch 8/70\n",
      " - 21s - loss: 0.6379 - acc: 0.7813 - val_loss: 0.7337 - val_acc: 0.7476\n",
      "Epoch 9/70\n",
      " - 21s - loss: 0.5981 - acc: 0.7934 - val_loss: 0.6025 - val_acc: 0.7915\n",
      "Epoch 10/70\n",
      " - 21s - loss: 0.5670 - acc: 0.8054 - val_loss: 0.7923 - val_acc: 0.7359\n",
      "Epoch 11/70\n",
      " - 21s - loss: 0.5390 - acc: 0.8142 - val_loss: 0.7814 - val_acc: 0.7462\n",
      "Epoch 12/70\n",
      " - 21s - loss: 0.5134 - acc: 0.8248 - val_loss: 0.5757 - val_acc: 0.8030\n",
      "Epoch 13/70\n",
      " - 21s - loss: 0.4881 - acc: 0.8346 - val_loss: 0.5799 - val_acc: 0.8050\n",
      "Epoch 14/70\n",
      " - 21s - loss: 0.4675 - acc: 0.8404 - val_loss: 0.5924 - val_acc: 0.8032\n",
      "Epoch 15/70\n",
      " - 21s - loss: 0.4439 - acc: 0.8488 - val_loss: 0.5869 - val_acc: 0.8058\n",
      "Epoch 16/70\n",
      " - 21s - loss: 0.4319 - acc: 0.8516 - val_loss: 0.7514 - val_acc: 0.7730\n",
      "Epoch 17/70\n",
      " - 21s - loss: 0.4110 - acc: 0.8589 - val_loss: 0.5956 - val_acc: 0.8091\n",
      "Epoch 18/70\n",
      " - 21s - loss: 0.3991 - acc: 0.8630 - val_loss: 0.5700 - val_acc: 0.8161\n",
      "Epoch 19/70\n",
      " - 21s - loss: 0.3849 - acc: 0.8679 - val_loss: 0.6431 - val_acc: 0.7876\n",
      "Epoch 20/70\n",
      " - 21s - loss: 0.3686 - acc: 0.8718 - val_loss: 0.4990 - val_acc: 0.8335\n",
      "Epoch 21/70\n",
      " - 21s - loss: 0.3540 - acc: 0.8774 - val_loss: 0.7618 - val_acc: 0.7655\n",
      "Epoch 22/70\n",
      " - 21s - loss: 0.3429 - acc: 0.8826 - val_loss: 0.5137 - val_acc: 0.8284\n",
      "Epoch 23/70\n",
      " - 21s - loss: 0.3344 - acc: 0.8849 - val_loss: 0.5301 - val_acc: 0.8299\n",
      "Epoch 24/70\n",
      " - 21s - loss: 0.3250 - acc: 0.8887 - val_loss: 0.6513 - val_acc: 0.8022\n",
      "Epoch 25/70\n",
      " - 21s - loss: 0.3061 - acc: 0.8955 - val_loss: 0.8451 - val_acc: 0.7595\n",
      "Epoch 26/70\n",
      " - 21s - loss: 0.3011 - acc: 0.8965 - val_loss: 0.5064 - val_acc: 0.8385\n",
      "Epoch 27/70\n",
      " - 21s - loss: 0.2903 - acc: 0.9001 - val_loss: 0.7283 - val_acc: 0.7893\n",
      "Epoch 28/70\n",
      " - 21s - loss: 0.2789 - acc: 0.9036 - val_loss: 0.5910 - val_acc: 0.8204\n",
      "Epoch 29/70\n",
      " - 21s - loss: 0.2744 - acc: 0.9058 - val_loss: 0.5365 - val_acc: 0.8331\n",
      "Epoch 30/70\n",
      " - 21s - loss: 0.2644 - acc: 0.9092 - val_loss: 0.4689 - val_acc: 0.8444\n",
      "Epoch 31/70\n",
      " - 21s - loss: 0.2616 - acc: 0.9086 - val_loss: 0.4640 - val_acc: 0.8504\n",
      "Epoch 32/70\n",
      " - 21s - loss: 0.2510 - acc: 0.9146 - val_loss: 0.5037 - val_acc: 0.8435\n",
      "Epoch 33/70\n",
      " - 21s - loss: 0.2436 - acc: 0.9156 - val_loss: 0.4264 - val_acc: 0.8593\n",
      "Epoch 34/70\n",
      " - 21s - loss: 0.2354 - acc: 0.9192 - val_loss: 0.4755 - val_acc: 0.8485\n",
      "Epoch 35/70\n",
      " - 21s - loss: 0.2230 - acc: 0.9220 - val_loss: 0.5117 - val_acc: 0.8427\n",
      "Epoch 36/70\n",
      " - 21s - loss: 0.2200 - acc: 0.9237 - val_loss: 0.5576 - val_acc: 0.8265\n",
      "Epoch 37/70\n",
      " - 21s - loss: 0.2172 - acc: 0.9248 - val_loss: 0.5331 - val_acc: 0.8420\n",
      "Epoch 38/70\n",
      " - 21s - loss: 0.2101 - acc: 0.9270 - val_loss: 0.5361 - val_acc: 0.8399\n",
      "Epoch 39/70\n",
      " - 21s - loss: 0.2023 - acc: 0.9304 - val_loss: 0.5287 - val_acc: 0.8440\n",
      "Epoch 40/70\n",
      " - 22s - loss: 0.1965 - acc: 0.9317 - val_loss: 0.5064 - val_acc: 0.8456\n",
      "Epoch 41/70\n",
      " - 22s - loss: 0.1943 - acc: 0.9321 - val_loss: 0.4666 - val_acc: 0.8533\n",
      "Epoch 42/70\n",
      " - 21s - loss: 0.1816 - acc: 0.9365 - val_loss: 0.3808 - val_acc: 0.8775\n",
      "Epoch 43/70\n",
      " - 22s - loss: 0.1750 - acc: 0.9391 - val_loss: 0.4756 - val_acc: 0.8568\n",
      "Epoch 44/70\n",
      " - 22s - loss: 0.1708 - acc: 0.9423 - val_loss: 0.4367 - val_acc: 0.8645\n",
      "Epoch 45/70\n",
      " - 21s - loss: 0.1650 - acc: 0.9434 - val_loss: 0.6402 - val_acc: 0.8248\n",
      "Epoch 46/70\n",
      " - 21s - loss: 0.1619 - acc: 0.9435 - val_loss: 0.4521 - val_acc: 0.8618\n",
      "Epoch 47/70\n",
      " - 22s - loss: 0.1541 - acc: 0.9466 - val_loss: 0.4362 - val_acc: 0.8685\n",
      "Epoch 48/70\n",
      " - 22s - loss: 0.1536 - acc: 0.9465 - val_loss: 0.4605 - val_acc: 0.8646\n",
      "Epoch 49/70\n",
      " - 22s - loss: 0.1490 - acc: 0.9489 - val_loss: 0.4217 - val_acc: 0.8749\n",
      "Epoch 50/70\n",
      " - 22s - loss: 0.1484 - acc: 0.9480 - val_loss: 0.4449 - val_acc: 0.8729\n",
      "Epoch 51/70\n",
      " - 22s - loss: 0.1393 - acc: 0.9527 - val_loss: 0.4085 - val_acc: 0.8761\n",
      "Epoch 52/70\n",
      " - 22s - loss: 0.1362 - acc: 0.9533 - val_loss: 0.4287 - val_acc: 0.8715\n",
      "Epoch 53/70\n",
      " - 22s - loss: 0.1320 - acc: 0.9556 - val_loss: 0.4992 - val_acc: 0.8607\n",
      "Epoch 54/70\n",
      " - 21s - loss: 0.1278 - acc: 0.9558 - val_loss: 0.4206 - val_acc: 0.8740\n",
      "Epoch 55/70\n",
      " - 22s - loss: 0.1272 - acc: 0.9565 - val_loss: 0.4073 - val_acc: 0.8774\n",
      "Epoch 56/70\n",
      " - 22s - loss: 0.1216 - acc: 0.9583 - val_loss: 0.4290 - val_acc: 0.8748\n",
      "Epoch 57/70\n",
      " - 22s - loss: 0.1181 - acc: 0.9599 - val_loss: 0.3997 - val_acc: 0.8810\n",
      "Epoch 58/70\n",
      " - 22s - loss: 0.1121 - acc: 0.9625 - val_loss: 0.3920 - val_acc: 0.8833\n",
      "Epoch 59/70\n",
      " - 22s - loss: 0.1083 - acc: 0.9630 - val_loss: 0.4021 - val_acc: 0.8802\n",
      "Epoch 60/70\n",
      " - 22s - loss: 0.1071 - acc: 0.9641 - val_loss: 0.3875 - val_acc: 0.8851\n",
      "Epoch 61/70\n",
      " - 22s - loss: 0.1051 - acc: 0.9646 - val_loss: 0.4092 - val_acc: 0.8796\n",
      "Epoch 62/70\n",
      " - 22s - loss: 0.1005 - acc: 0.9657 - val_loss: 0.3956 - val_acc: 0.8807\n",
      "Epoch 63/70\n",
      " - 22s - loss: 0.0985 - acc: 0.9672 - val_loss: 0.3999 - val_acc: 0.8798\n",
      "Epoch 64/70\n",
      " - 22s - loss: 0.0978 - acc: 0.9684 - val_loss: 0.3772 - val_acc: 0.8871\n",
      "Epoch 65/70\n",
      " - 22s - loss: 0.0942 - acc: 0.9685 - val_loss: 0.4114 - val_acc: 0.8808\n",
      "Epoch 66/70\n",
      " - 20s - loss: 0.0908 - acc: 0.9714 - val_loss: 0.3756 - val_acc: 0.8873\n",
      "Epoch 67/70\n",
      " - 20s - loss: 0.0893 - acc: 0.9707 - val_loss: 0.3821 - val_acc: 0.8842\n",
      "Epoch 68/70\n",
      " - 20s - loss: 0.0880 - acc: 0.9714 - val_loss: 0.3834 - val_acc: 0.8850\n",
      "Epoch 69/70\n",
      " - 20s - loss: 0.0859 - acc: 0.9723 - val_loss: 0.3707 - val_acc: 0.8878\n",
      "Epoch 70/70\n",
      " - 20s - loss: 0.0859 - acc: 0.9712 - val_loss: 0.3689 - val_acc: 0.8887\n"
     ]
    }
   ],
   "source": [
    "# initialize the optimizer and model\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = SGD(lr=INIT_LR, momentum=0.9)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "\tmetrics=[\"accuracy\"])\n",
    " \n",
    "# train the network\n",
    "print(\"[INFO] training network...\")\n",
    "H = model.fit_generator(\n",
    "\taug.flow(trainX, trainY, batch_size=64 * G),\n",
    "\tvalidation_data=(testX, testY),\n",
    "\tsteps_per_epoch=len(trainX) // (64 * G),\n",
    "\tepochs=NUM_EPOCHS,\n",
    "\tcallbacks=callbacks, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# grab the history object dictionary\n",
    "#H = H['history']\n",
    " \n",
    "# plot the training loss and accuracy\n",
    "N = np.arange(0, len(H[\"loss\"]))\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(N, H[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, H[\"val_loss\"], label=\"test_loss\")\n",
    "plt.plot(N, H[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(N, H[\"val_acc\"], label=\"test_acc\")\n",
    "plt.title(\"MiniGoogLeNet on CIFAR-10\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()\n",
    " \n",
    "# save the figure\n",
    "plt.savefig(output)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
