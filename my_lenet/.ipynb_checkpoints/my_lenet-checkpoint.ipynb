{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3e9ca5e41e1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimagenet_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import datasets\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "import h5py\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1)Load dataset\n",
    "dataset = datasets.fetch_mldata(\"MNIST Original\")\n",
    "print(dataset.data[1].shape)\n",
    "data = dataset.data.reshape((dataset.data.shape[0], 28, 28))\n",
    "print(data[0].shape)\n",
    "data = data[:, np.newaxis, :, :]\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2)Dataset partition int train and test data\n",
    "#splits data into train and test by using test split of 33%\n",
    "#using split method, and normalizes the pixel values\n",
    "(trainData, testData, trainLabels, testLabels) = train_test_split(\n",
    "    data / 255.0, dataset.target.astype(\"int\"), test_size=0.33)\n",
    "print(trainData.shape, trainLabels, testData.shape, testLabels)\n",
    "print(trainLabels)\n",
    "# transform the training and testing labels into vectors in the\n",
    "# range [0, classes] -- this generates a vector for each label,\n",
    "# where the index of the label is set to `1` and all other entries\n",
    "# to `0`; in the case of MNIST, there are 10 class labels\n",
    "trainLabels = np_utils.to_categorical(trainLabels, 10)\n",
    "print(trainLabels[0])\n",
    "testLabels = np_utils.to_categorical(testLabels, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3)Loading model architecture\n",
    "(width, height, depth, classes) = (28, 28, 1, 10)\n",
    "print(width, height, depth, classes)\n",
    "opt = SGD(lr=0.01) #optimizer\n",
    "\n",
    "model = Sequential()#one layer after the other\n",
    "\n",
    "# first set of CONV => RELU => POOL\n",
    "model.add(Convolution2D(20, (5, 5), padding=\"same\",\n",
    "                        input_shape=(depth, height, width),\n",
    "                        data_format=\"channels_first\"))#you only have to declare data format on the first layer\n",
    "model.add(Activation(\"relu\"))#activation\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))#maxpooling--> 2x2 sliding window that walks a distance of 2 pixels to x and y\n",
    "\n",
    "# second set of CONV => RELU => POOL\n",
    "model.add(Convolution2D(50, (5, 5), padding=\"same\")) #extracts 50 5x5 filters, same input\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# set of FC => RELU layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "# softmax classifier--> only in the end. Its the NN\n",
    "model.add(Dense(classes))\n",
    "model.add(Activation(\"softmax\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#4)Compiling model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "              metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#5)Training model\n",
    "model.fit(trainData, trainLabels, batch_size=128, nb_epoch=20,\n",
    "          verbose=1)\n",
    "\n",
    "(loss, accuracy) = model.evaluate(testData, testLabels,\n",
    "                                  batch_size=128, verbose=1)\n",
    "print(\"[INFO] accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "model = Sequential()#one layer after the other\n",
    "model.save_weights(\"lenet_weights.hdf5\", overwrite=True)\n",
    "\n",
    "model_json = model.to_json()\n",
    "print(model_json)\n",
    "with open(\"my_model.json\", \"w+\") as json_file:\n",
    "    json_file.write(model_json)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
